{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# copilot_ET_comprehensive_ai_usage_analysis\n",
    "\n",
    "This notebook is an improved, safer, and more reproducible version of \"ET_comprehensive_ai_usage_analysis.ipynb\".\n",
    "It includes robust data loading, guarded statistical tests, feature engineering, and an ML pipeline with cross-validation and grid search.\n",
    "\n",
    "**Author**: Copilot (created for EmanToraih-AI)\n",
    "**Date**: 2026-01-05\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "Run this cell to (optionally) install required packages in the notebook environment.\n",
    "If your environment already provides these packages you can skip installation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install missing packages in some environments\n",
    "# !pip install -q scikit-learn xgboost lightgbm shap joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports and configuration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score)\n",
    "import joblib\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 5)\n",
    "\n",
    "print('Imports ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust data loader and helpers\n",
    "REQUIRED_COLUMNS = [\n",
    "    'SessionID', 'SessionDate', 'UsedAgain', 'StudentLevel', 'TaskType',\n",
    "    'Discipline', 'FinalOutcome', 'SessionLengthMin', 'TotalPrompts',\n",
    "    'AI_AssistanceLevel', 'SatisfactionRating'\n",
    "]\n",
    "\n",
    "def load_data(path='ai_assistant_usage_student_life.csv'):\n",
    "    df = pd.read_csv(path)\n",
    "    missing = [c for c in REQUIRED_COLUMNS if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f'Missing required columns: {missing}')\n",
    "    # Parse dates safely\n",
    "    df['SessionDate'] = pd.to_datetime(df['SessionDate'], errors='coerce')\n",
    "    if df['SessionDate'].isna().any():\n",
    "        print('Warning: Some SessionDate values could not be parsed and are NaT')\n",
    "    # Standardize UsedAgain to boolean (support 0/1/'True'/'False')\n",
    "    df['UsedAgain'] = df['UsedAgain'].map({1: True, 0: False, '1': True, '0': False, 'True': True, 'False': False}).fillna(df['UsedAgain'])\n",
    "    df['UsedAgain'] = df['UsedAgain'].astype(bool)\n",
    "    return df\n",
    "\n",
    "def safe_value_counts_bool(series):\n",
    "    counts = series.value_counts()\n",
    "    return int(counts.get(True, 0)), int(counts.get(False, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = load_data('ai_assistant_usage_student_life.csv')\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safe UsedAgain distribution and plot\n",
    "true_count, false_count = safe_value_counts_bool(df['UsedAgain'])\n",
    "total = len(df)\n",
    "print(f'UsedAgain: True={true_count}, False={false_count}, total={total}')\n",
    "print(f'Percentages -> True: {true_count/total*100:.2f}%, False: {false_count/total*100:.2f}%')\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax[0].bar(['True','False'], [true_count, false_count], color=['#51cf66','#ff6b6b'])\n",
    "ax[0].set_title('UsedAgain counts')\n",
    "ax[1].pie([true_count, false_count], labels=[f'True ({true_count/total*100:.1f}%)', f'False ({false_count/total*100:.1f}%)'],\n",
    "          colors=['#51cf66','#ff6b6b'], startangle=90)\n",
    "ax[1].set_title('UsedAgain percentage')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering (safe)\n",
    "df_fe = df.copy()\n",
    "# Efficiency metrics\n",
    "df_fe['MinPerPrompt'] = df_fe['SessionLengthMin'] / df_fe['TotalPrompts']\n",
    "df_fe['MinPerPrompt'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_fe['SatisfactionPerPrompt'] = df_fe['SatisfactionRating'] / df_fe['TotalPrompts']\n",
    "df_fe['SatisfactionPerPrompt'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_fe['PromptsPerMin'] = df_fe['TotalPrompts'] / df_fe['SessionLengthMin']\n",
    "df_fe['PromptsPerMin'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# Binary features\n",
    "df_fe['HighAssistance'] = (df_fe['AI_AssistanceLevel'] >= 3).astype(int)\n",
    "df_fe['HighSatisfaction'] = (df_fe['SatisfactionRating'] >= 4).astype(int)\n",
    "df_fe['LongSession'] = (df_fe['SessionLengthMin'] >= df_fe['SessionLengthMin'].median()).astype(int)\n",
    "df_fe['SuccessfulOutcome'] = (df_fe['FinalOutcome'] == 'Assignment Completed').astype(int)\n",
    "\n",
    "display(df_fe[['MinPerPrompt','SatisfactionPerPrompt','PromptsPerMin']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for ML using ColumnTransformer + Pipeline\n",
    "feature_cols = ['StudentLevel','Discipline','TaskType','FinalOutcome',\n",
    "                'SessionLengthMin','TotalPrompts','AI_AssistanceLevel','SatisfactionRating',\n",
    "                'MinPerPrompt','SatisfactionPerPrompt','PromptsPerMin',\n",
    "                'HighAssistance','HighSatisfaction','LongSession','SuccessfulOutcome']\n",
    "X = df_fe[feature_cols].copy()\n",
    "y = df_fe['UsedAgain'].astype(int)\n",
    "\n",
    "categorical_cols = ['StudentLevel','Discipline','TaskType','FinalOutcome']\n",
    "numeric_cols = [c for c in feature_cols if c not in categorical_cols]  # keep engineered numeric columns\n",
    "\n",
    "# Column transformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), numeric_cols),\n",
    "    ('cat', OneHotEncoder(drop='first', sparse=False, handle_unknown='ignore'), categorical_cols)\n",
    "], remainder='drop')\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('pre', preprocessor),\n",
    "    ('clf', RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "print('Pipeline ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated grid search (example)\n",
    "param_grid = {\n",
    "    'clf__n_estimators': [100, 200],\n",
    "    'clf__max_depth': [None, 10, 20]\n",
    "}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid = GridSearchCV(pipe, param_grid, cv=cv, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit on the entire dataset (you may prefer to split train/test first)\n",
    "grid.fit(X, y)\n",
    "print('Best CV ROC-AUC:', grid.best_score_)\n",
    "print('Best params:', grid.best_params_)\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Evaluate on a held-out test set for a final estimate\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:,1]\n",
    "print('Test Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Test Precision:', precision_score(y_test, y_pred))\n",
    "print('Test Recall:', recall_score(y_test, y_pred))\n",
    "print('Test F1:', f1_score(y_test, y_pred))\n",
    "print('Test ROC-AUC:', roc_auc_score(y_test, y_proba))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to disk\n",
    "joblib.dump(best_model, 'copilot_best_model.joblib')\n",
    "print('Saved model to copilot_best_model.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes and next steps\n",
    "\n",
    "- This notebook focuses on safety, reproducibility, and a robust ML pipeline.\n",
    "- Consider adding SHAP explainability for the final tree-based model (install shap).\n",
    "- Consider handling missing values explicitly (imputation) before modeling for production usage.\n",
    "- If the dataset is imbalanced, try resampling (SMOTE) or threshold tuning in addition to class_weight.\n",
    "\n",
    "You can compare this notebook to the original and adapt additional analysis cells as needed.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
