{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Analysis: AI Assistant Usage in Student Life\n",
    "\n",
    "**Author**: Eman Toraih  \n",
    "**Date**: January 5, 2026  \n",
    "**Purpose**: Comprehensive statistical analysis and machine learning modeling of AI assistant usage patterns in student life\n",
    "\n",
    "---\n",
    "\n",
    "## Research Objectives\n",
    "This comprehensive analysis explores patterns, predictors, and insights from student interactions with AI assistants. The analysis aims to:\n",
    "- Understand usage patterns across student demographics and task types\n",
    "- Identify factors that predict AI tool reuse\n",
    "- Conduct statistical hypothesis testing\n",
    "- Build predictive models for publication\n",
    "- Generate actionable insights for educators and developers\n",
    "\n",
    "## Dataset Overview\n",
    "- **Source**: Kaggle - AI Assistant Usage in Student Life (Synthetic Dataset)\n",
    "- **Sample Size**: 10,000 student-AI interaction sessions\n",
    "- **Variables**: 11 features including demographics, session characteristics, and outcomes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Library Imports and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Statistical Analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, f_oneway, mannwhitneyu\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                            roc_auc_score, roc_curve, confusion_matrix, classification_report,\n",
    "                            precision_recall_curve, auc)\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Screening\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('ai_assistant_usage_student_life.csv')\n",
    "\n",
    "# Convert SessionDate to datetime\n",
    "df['SessionDate'] = pd.to_datetime(df['SessionDate'])\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data inspection\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\n\\nDataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n\\nBasic Statistics:\")\n",
    "display(df.describe())\n",
    "\n",
    "print(\"\\n\\nData Types:\")\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Check\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. Missing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Missing Percentage': missing_pct\n",
    "})\n",
    "display(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "print(\"\\n2. Duplicate Rows:\")\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Total duplicates: {duplicates} ({duplicates/len(df)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n3. Unique Values per Categorical Column:\")\n",
    "categorical_cols = df.select_dtypes(include=['object', 'bool']).columns\n",
    "for col in categorical_cols:\n",
    "    if col != 'SessionID':\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(df[col].value_counts())\n",
    "        print(f\"Unique values: {df[col].nunique()}\")\n",
    "\n",
    "print(\"\\n4. Numerical Column Ranges:\")\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "for col in numerical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Min: {df[col].min()}, Max: {df[col].max()}\")\n",
    "    print(f\"  Mean: {df[col].mean():.2f}, Median: {df[col].median():.2f}\")\n",
    "    print(f\"  Std: {df[col].std():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 3.1 Target Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable: UsedAgain\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "used_again_counts = df['UsedAgain'].value_counts()\n",
    "axes[0].bar(used_again_counts.index.astype(str), used_again_counts.values, \n",
    "            color=['#ff6b6b', '#51cf66'], edgecolor='black', linewidth=1.5)\n",
    "axes[0].set_xlabel('Used Again', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Distribution of UsedAgain Variable', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count labels on bars\n",
    "for i, v in enumerate(used_again_counts.values):\n",
    "    axes[0].text(i, v + 50, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "used_again_pct = df['UsedAgain'].value_counts(normalize=True) * 100\n",
    "colors = ['#ff6b6b', '#51cf66']\n",
    "axes[1].pie(used_again_pct.values, labels=[f'{label}\\\\n({pct:.1f}%)' \n",
    "                                           for label, pct in zip(used_again_pct.index, used_again_pct.values)],\n",
    "            autopct='', colors=colors, startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "axes[1].set_title('UsedAgain Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"UsedAgain Statistics:\")\n",
    "print(f\"True: {used_again_counts[True]:,} ({used_again_pct[True]:.2f}%)\")\n",
    "print(f\"False: {used_again_counts[False]:,} ({used_again_pct[False]:.2f}%)\")\n",
    "print(f\"\\nClass Imbalance Ratio: {used_again_counts[False]/used_again_counts[True]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Categorical Variables Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student Level distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Student Level\n",
    "student_level_counts = df['StudentLevel'].value_counts()\n",
    "axes[0, 0].barh(student_level_counts.index, student_level_counts.values, color='#339af0', edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('Distribution by Student Level', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(axis='x', alpha=0.3)\n",
    "for i, v in enumerate(student_level_counts.values):\n",
    "    axes[0, 0].text(v + 50, i, str(v), va='center', fontweight='bold')\n",
    "\n",
    "# Task Type\n",
    "task_type_counts = df['TaskType'].value_counts()\n",
    "axes[0, 1].barh(task_type_counts.index, task_type_counts.values, color='#51cf66', edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('Distribution by Task Type', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(axis='x', alpha=0.3)\n",
    "for i, v in enumerate(task_type_counts.values):\n",
    "    axes[0, 1].text(v + 50, i, str(v), va='center', fontweight='bold')\n",
    "\n",
    "# Discipline (top 10)\n",
    "discipline_counts = df['Discipline'].value_counts().head(10)\n",
    "axes[1, 0].barh(discipline_counts.index, discipline_counts.values, color='#ff922b', edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title('Top 10 Disciplines', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "for i, v in enumerate(discipline_counts.values):\n",
    "    axes[1, 0].text(v + 50, i, str(v), va='center', fontweight='bold')\n",
    "\n",
    "# Final Outcome\n",
    "outcome_counts = df['FinalOutcome'].value_counts()\n",
    "axes[1, 1].barh(outcome_counts.index, outcome_counts.values, color='#ae3ec9', edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('Distribution by Final Outcome', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(axis='x', alpha=0.3)\n",
    "for i, v in enumerate(outcome_counts.values):\n",
    "    axes[1, 1].text(v + 50, i, str(v), va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Numerical Variables Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of numerical variables\n",
    "numerical_cols = ['SessionLengthMin', 'TotalPrompts', 'AI_AssistanceLevel', 'SatisfactionRating']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    row = idx // 2\n",
    "    col_idx = idx % 2\n",
    "    \n",
    "    # Histogram with KDE\n",
    "    axes[row, col_idx].hist(df[col], bins=50, color='#339af0', alpha=0.7, edgecolor='black')\n",
    "    axes[row, col_idx].axvline(df[col].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df[col].mean():.2f}')\n",
    "    axes[row, col_idx].axvline(df[col].median(), color='green', linestyle='--', linewidth=2, label=f'Median: {df[col].median():.2f}')\n",
    "    axes[row, col_idx].set_xlabel(col, fontsize=12, fontweight='bold')\n",
    "    axes[row, col_idx].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "    axes[row, col_idx].set_title(f'Distribution of {col}', fontsize=14, fontweight='bold')\n",
    "    axes[row, col_idx].legend()\n",
    "    axes[row, col_idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Box plots\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    bp = axes[idx].boxplot(df[col], patch_artist=True, \n",
    "                           boxprops=dict(facecolor='#339af0', alpha=0.7),\n",
    "                           medianprops=dict(color='red', linewidth=2))\n",
    "    axes[idx].set_ylabel(col, fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_title(f'Box Plot: {col}', fontsize=14, fontweight='bold')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Correlation Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numerical variables\n",
    "numerical_df = df[['SessionLengthMin', 'TotalPrompts', 'AI_AssistanceLevel', \n",
    "                   'SatisfactionRating']].copy()\n",
    "numerical_df['UsedAgain'] = df['UsedAgain'].astype(int)\n",
    "\n",
    "correlation_matrix = numerical_df.corr()\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.3f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, mask=mask,\n",
    "            annot_kws={'fontsize': 11, 'fontweight': 'bold'})\n",
    "plt.title('Correlation Matrix of Numerical Variables', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print strong correlations\n",
    "print(\"Strong Correlations (|r| > 0.5):\")\n",
    "print(\"=\" * 50)\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_val = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.5:\n",
    "            print(f\"{correlation_matrix.columns[i]} vs {correlation_matrix.columns[j]}: {corr_val:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Relationship Between Categorical Variables and UsedAgain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze UsedAgain by categorical variables\n",
    "categorical_vars = ['StudentLevel', 'TaskType', 'FinalOutcome']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for idx, var in enumerate(categorical_vars):\n",
    "    # Calculate percentages\n",
    "    crosstab = pd.crosstab(df[var], df['UsedAgain'], normalize='index') * 100\n",
    "    \n",
    "    # Create stacked bar chart\n",
    "    crosstab.plot(kind='bar', stacked=True, ax=axes[idx], \n",
    "                  color=['#ff6b6b', '#51cf66'], edgecolor='black', linewidth=1)\n",
    "    axes[idx].set_xlabel(var, fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Percentage (%)', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_title(f'UsedAgain Distribution by {var}', fontsize=14, fontweight='bold')\n",
    "    axes[idx].legend(['False', 'True'], title='Used Again', fontsize=10)\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed statistics\n",
    "for var in categorical_vars:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"UsedAgain by {var}\")\n",
    "    print('='*60)\n",
    "    crosstab_counts = pd.crosstab(df[var], df['UsedAgain'], margins=True)\n",
    "    crosstab_pct = pd.crosstab(df[var], df['UsedAgain'], normalize='index') * 100\n",
    "    print(\"\\nCounts:\")\n",
    "    display(crosstab_counts)\n",
    "    print(\"\\nPercentages:\")\n",
    "    display(crosstab_pct.round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Relationship Between Numerical Variables and UsedAgain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare numerical variables by UsedAgain\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    row = idx // 2\n",
    "    col_idx = idx % 2\n",
    "    \n",
    "    # Create grouped box plot\n",
    "    data_to_plot = [df[df['UsedAgain']==False][col], df[df['UsedAgain']==True][col]]\n",
    "    bp = axes[row, col_idx].boxplot(data_to_plot, labels=['False', 'True'],\n",
    "                                    patch_artist=True,\n",
    "                                    boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                                    medianprops=dict(color='red', linewidth=2))\n",
    "    axes[row, col_idx].set_xlabel('Used Again', fontsize=12, fontweight='bold')\n",
    "    axes[row, col_idx].set_ylabel(col, fontsize=12, fontweight='bold')\n",
    "    axes[row, col_idx].set_title(f'{col} by UsedAgain', fontsize=14, fontweight='bold')\n",
    "    axes[row, col_idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nStatistical Summary by UsedAgain:\")\n",
    "print(\"=\"*70)\n",
    "for col in numerical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    summary = df.groupby('UsedAgain')[col].agg(['mean', 'median', 'std', 'min', 'max'])\n",
    "    display(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Hypothesis Testing\n",
    "\n",
    "### 4.1 Chi-Square Tests for Categorical Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-square tests for independence\n",
    "categorical_vars = ['StudentLevel', 'TaskType', 'FinalOutcome', 'Discipline']\n",
    "\n",
    "chi2_results = []\n",
    "\n",
    "for var in categorical_vars:\n",
    "    contingency_table = pd.crosstab(df[var], df['UsedAgain'])\n",
    "    chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    # Cramer's V for effect size\n",
    "    n = contingency_table.sum().sum()\n",
    "    cramers_v = np.sqrt(chi2 / (n * (min(contingency_table.shape) - 1)))\n",
    "    \n",
    "    chi2_results.append({\n",
    "        'Variable': var,\n",
    "        'Chi-square': chi2,\n",
    "        'p-value': p_value,\n",
    "        'df': dof,\n",
    "        \"Cramer's V\": cramers_v,\n",
    "        'Significant': 'Yes' if p_value < 0.05 else 'No'\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Chi-square Test: {var} vs UsedAgain\")\n",
    "    print('='*60)\n",
    "    print(f\"Chi-square statistic: {chi2:.4f}\")\n",
    "    print(f\"Degrees of freedom: {dof}\")\n",
    "    print(f\"p-value: {p_value:.6f}\")\n",
    "    print(f\"Cramer's V: {cramers_v:.4f}\")\n",
    "    print(f\"Significant at \u03b1=0.05: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "    if p_value < 0.001:\n",
    "        print(\"Interpretation: Very strong evidence of association\")\n",
    "    elif p_value < 0.01:\n",
    "        print(\"Interpretation: Strong evidence of association\")\n",
    "    elif p_value < 0.05:\n",
    "        print(\"Interpretation: Evidence of association\")\n",
    "    else:\n",
    "        print(\"Interpretation: No significant association\")\n",
    "\n",
    "chi2_df = pd.DataFrame(chi2_results)\n",
    "print(\"\\n\\nSummary of Chi-square Tests:\")\n",
    "print(\"=\"*70)\n",
    "display(chi2_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 T-tests and Non-parametric Tests for Numerical Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare numerical variables between UsedAgain groups\n",
    "numerical_cols = ['SessionLengthMin', 'TotalPrompts', 'AI_AssistanceLevel', 'SatisfactionRating']\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for col in numerical_cols:\n",
    "    group_false = df[df['UsedAgain']==False][col].dropna()\n",
    "    group_true = df[df['UsedAgain']==True][col].dropna()\n",
    "    \n",
    "    # Check normality (Shapiro-Wilk test on smaller sample for speed)\n",
    "    sample_size = min(5000, len(group_false), len(group_true))\n",
    "    _, p_norm_false = stats.shapiro(group_false.sample(sample_size)) if len(group_false) > 3 else (None, 0.01)\n",
    "    _, p_norm_true = stats.shapiro(group_true.sample(sample_size)) if len(group_true) > 3 else (None, 0.01)\n",
    "    is_normal = p_norm_false > 0.05 and p_norm_true > 0.05\n",
    "    \n",
    "    # Welch's t-test (doesn't assume equal variances)\n",
    "    t_stat, p_t = stats.ttest_ind(group_true, group_false, equal_var=False)\n",
    "    \n",
    "    # Mann-Whitney U test (non-parametric)\n",
    "    u_stat, p_u = mannwhitneyu(group_true, group_false, alternative='two-sided')\n",
    "    \n",
    "    # Effect size (Cohen's d)\n",
    "    pooled_std = np.sqrt(((len(group_true)-1)*group_true.std()**2 + \n",
    "                          (len(group_false)-1)*group_false.std()**2) / \n",
    "                         (len(group_true) + len(group_false) - 2))\n",
    "    cohens_d = (group_true.mean() - group_false.mean()) / pooled_std\n",
    "    \n",
    "    test_results.append({\n",
    "        'Variable': col,\n",
    "        'UsedAgain=True Mean': group_true.mean(),\n",
    "        'UsedAgain=False Mean': group_false.mean(),\n",
    "        'Mean Difference': group_true.mean() - group_false.mean(),\n",
    "        'T-statistic': t_stat,\n",
    "        'T-test p-value': p_t,\n",
    "        'Mann-Whitney U': u_stat,\n",
    "        'MW p-value': p_u,\n",
    "        \"Cohen's d\": cohens_d,\n",
    "        'Normal Distribution': 'Yes' if is_normal else 'No',\n",
    "        'Significant (t-test)': 'Yes' if p_t < 0.05 else 'No'\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Statistical Tests: {col} by UsedAgain\")\n",
    "    print('='*70)\n",
    "    print(f\"UsedAgain=True:  Mean={group_true.mean():.3f}, SD={group_true.std():.3f}, N={len(group_true)}\")\n",
    "    print(f\"UsedAgain=False: Mean={group_false.mean():.3f}, SD={group_false.std():.3f}, N={len(group_false)}\")\n",
    "    print(f\"\\nWelch's t-test: t={t_stat:.4f}, p={p_t:.6f}\")\n",
    "    print(f\"Mann-Whitney U: U={u_stat:.4f}, p={p_u:.6f}\")\n",
    "    print(f\"Cohen's d: {cohens_d:.4f}\")\n",
    "    \n",
    "    if abs(cohens_d) < 0.2:\n",
    "        effect_size = \"negligible\"\n",
    "    elif abs(cohens_d) < 0.5:\n",
    "        effect_size = \"small\"\n",
    "    elif abs(cohens_d) < 0.8:\n",
    "        effect_size = \"medium\"\n",
    "    else:\n",
    "        effect_size = \"large\"\n",
    "    print(f\"Effect size: {effect_size}\")\n",
    "    \n",
    "    if p_t < 0.05:\n",
    "        print(f\"Interpretation: Significant difference (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"Interpretation: No significant difference (p \u2265 0.05)\")\n",
    "\n",
    "test_df = pd.DataFrame(test_results)\n",
    "print(\"\\n\\nSummary of Statistical Tests:\")\n",
    "print(\"=\"*70)\n",
    "display(test_df.round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 ANOVA for Multiple Groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA for numerical variables across categorical groups\n",
    "anova_results = []\n",
    "\n",
    "# Test SatisfactionRating across TaskType\n",
    "groups_task = [df[df['TaskType']==task]['SatisfactionRating'].dropna() \n",
    "               for task in df['TaskType'].unique()]\n",
    "f_stat_task, p_anova_task = f_oneway(*groups_task)\n",
    "eta_squared_task = f_stat_task / (f_stat_task + (len(df) - len(df['TaskType'].unique())))\n",
    "\n",
    "anova_results.append({\n",
    "    'Variable': 'SatisfactionRating',\n",
    "    'Group Variable': 'TaskType',\n",
    "    'F-statistic': f_stat_task,\n",
    "    'p-value': p_anova_task,\n",
    "    'Eta-squared': eta_squared_task,\n",
    "    'Significant': 'Yes' if p_anova_task < 0.05 else 'No'\n",
    "})\n",
    "\n",
    "print(\"ANOVA: SatisfactionRating across TaskType\")\n",
    "print(\"=\"*60)\n",
    "print(f\"F-statistic: {f_stat_task:.4f}\")\n",
    "print(f\"p-value: {p_anova_task:.6f}\")\n",
    "print(f\"Eta-squared: {eta_squared_task:.4f}\")\n",
    "print(f\"Significant at \u03b1=0.05: {'Yes' if p_anova_task < 0.05 else 'No'}\")\n",
    "\n",
    "# Test SessionLengthMin across StudentLevel\n",
    "groups_level = [df[df['StudentLevel']==level]['SessionLengthMin'].dropna() \n",
    "                for level in df['StudentLevel'].unique()]\n",
    "f_stat_level, p_anova_level = f_oneway(*groups_level)\n",
    "eta_squared_level = f_stat_level / (f_stat_level + (len(df) - len(df['StudentLevel'].unique())))\n",
    "\n",
    "anova_results.append({\n",
    "    'Variable': 'SessionLengthMin',\n",
    "    'Group Variable': 'StudentLevel',\n",
    "    'F-statistic': f_stat_level,\n",
    "    'p-value': p_anova_level,\n",
    "    'Eta-squared': eta_squared_level,\n",
    "    'Significant': 'Yes' if p_anova_level < 0.05 else 'No'\n",
    "})\n",
    "\n",
    "print(\"\\n\\nANOVA: SessionLengthMin across StudentLevel\")\n",
    "print(\"=\"*60)\n",
    "print(f\"F-statistic: {f_stat_level:.4f}\")\n",
    "print(f\"p-value: {p_anova_level:.6f}\")\n",
    "print(f\"Eta-squared: {eta_squared_level:.4f}\")\n",
    "print(f\"Significant at \u03b1=0.05: {'Yes' if p_anova_level < 0.05 else 'No'}\")\n",
    "\n",
    "anova_df = pd.DataFrame(anova_results)\n",
    "print(\"\\n\\nSummary of ANOVA Tests:\")\n",
    "print(\"=\"*70)\n",
    "display(anova_df.round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Time Series Analysis\n",
    "\n",
    "### 5.1 Temporal Trends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract temporal features\n",
    "df['Year'] = df['SessionDate'].dt.year\n",
    "df['Month'] = df['SessionDate'].dt.month\n",
    "df['DayOfWeek'] = df['SessionDate'].dt.day_name()\n",
    "df['WeekOfYear'] = df['SessionDate'].dt.isocalendar().week\n",
    "\n",
    "# Time series analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# Sessions over time (monthly)\n",
    "monthly_counts = df.groupby([df['SessionDate'].dt.to_period('M')]).size()\n",
    "axes[0, 0].plot(monthly_counts.index.astype(str), monthly_counts.values, \n",
    "                marker='o', linewidth=2, markersize=8, color='#339af0')\n",
    "axes[0, 0].set_xlabel('Month', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Number of Sessions', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('Total Sessions Over Time (Monthly)', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# UsedAgain rate over time\n",
    "monthly_used = df.groupby([df['SessionDate'].dt.to_period('M'), 'UsedAgain']).size().unstack(fill_value=0)\n",
    "monthly_used_pct = (monthly_used[True] / (monthly_used[True] + monthly_used[False]) * 100)\n",
    "axes[0, 1].plot(monthly_used_pct.index.astype(str), monthly_used_pct.values,\n",
    "                marker='o', linewidth=2, markersize=8, color='#51cf66')\n",
    "axes[0, 1].set_xlabel('Month', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('UsedAgain Rate (%)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('UsedAgain Rate Over Time', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "axes[0, 1].axhline(y=monthly_used_pct.mean(), color='red', linestyle='--', \n",
    "                   label=f'Mean: {monthly_used_pct.mean():.1f}%')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Sessions by day of week\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "day_counts = df['DayOfWeek'].value_counts().reindex(day_order, fill_value=0)\n",
    "axes[1, 0].bar(range(len(day_counts)), day_counts.values, color='#ff922b', edgecolor='black')\n",
    "axes[1, 0].set_xticks(range(len(day_counts)))\n",
    "axes[1, 0].set_xticklabels(day_counts.index, rotation=45, ha='right')\n",
    "axes[1, 0].set_ylabel('Number of Sessions', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title('Sessions by Day of Week', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Average SatisfactionRating over time\n",
    "monthly_satisfaction = df.groupby(df['SessionDate'].dt.to_period('M'))['SatisfactionRating'].mean()\n",
    "axes[1, 1].plot(monthly_satisfaction.index.astype(str), monthly_satisfaction.values,\n",
    "                marker='o', linewidth=2, markersize=8, color='#ae3ec9')\n",
    "axes[1, 1].set_xlabel('Month', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Average Satisfaction Rating', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('Average Satisfaction Rating Over Time', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "axes[1, 1].axhline(y=monthly_satisfaction.mean(), color='red', linestyle='--',\n",
    "                   label=f'Mean: {monthly_satisfaction.mean():.2f}')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Machine Learning Models\n",
    "\n",
    "### 7.1 Data Preparation for ML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for feature engineering\n",
    "df_fe = df.copy()\n",
    "\n",
    "# Feature engineering\n",
    "print(\"Creating new features...\")\n",
    "\n",
    "# 1. Efficiency metrics\n",
    "df_fe['MinPerPrompt'] = df_fe['SessionLengthMin'] / df_fe['TotalPrompts']\n",
    "df_fe['MinPerPrompt'] = df_fe['MinPerPrompt'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 2. Satisfaction per prompt\n",
    "df_fe['SatisfactionPerPrompt'] = df_fe['SatisfactionRating'] / df_fe['TotalPrompts']\n",
    "df_fe['SatisfactionPerPrompt'] = df_fe['SatisfactionPerPrompt'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 3. Session intensity (prompts per minute)\n",
    "df_fe['PromptsPerMin'] = df_fe['TotalPrompts'] / df_fe['SessionLengthMin']\n",
    "df_fe['PromptsPerMin'] = df_fe['PromptsPerMin'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 4. Binary features\n",
    "df_fe['HighAssistance'] = (df_fe['AI_AssistanceLevel'] >= 3).astype(int)\n",
    "df_fe['HighSatisfaction'] = (df_fe['SatisfactionRating'] >= 4).astype(int)\n",
    "df_fe['LongSession'] = (df_fe['SessionLengthMin'] >= df_fe['SessionLengthMin'].median()).astype(int)\n",
    "\n",
    "# 5. Outcome success\n",
    "df_fe['SuccessfulOutcome'] = (df_fe['FinalOutcome'] == 'Assignment Completed').astype(int)\n",
    "\n",
    "# 6. Temporal features (already created)\n",
    "# Year, Month, DayOfWeek, WeekOfYear\n",
    "\n",
    "print(f\"Original features: {len(df.columns)}\")\n",
    "print(f\"Total features after engineering: {len(df_fe.columns)}\")\n",
    "print(f\"\\nNew features created:\")\n",
    "new_features = [col for col in df_fe.columns if col not in df.columns]\n",
    "for feat in new_features:\n",
    "    print(f\"  - {feat}\")\n",
    "\n",
    "# Display summary of new numerical features\n",
    "print(\"\\n\\nSummary of new numerical features:\")\n",
    "new_num_features = [f for f in new_features if df_fe[f].dtype in [np.float64, np.int64]]\n",
    "display(df_fe[new_num_features].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Machine Learning Models\n",
    "\n",
    "### 7.1 Data Preparation for ML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for machine learning\n",
    "# Select features for modeling\n",
    "feature_cols = ['StudentLevel', 'Discipline', 'TaskType', 'FinalOutcome',\n",
    "                'SessionLengthMin', 'TotalPrompts', 'AI_AssistanceLevel', \n",
    "                'SatisfactionRating', 'MinPerPrompt', 'SatisfactionPerPrompt',\n",
    "                'HighAssistance', 'HighSatisfaction', 'LongSession', 'SuccessfulOutcome']\n",
    "\n",
    "# Create feature dataframe\n",
    "X = df_fe[feature_cols].copy()\n",
    "y = df_fe['UsedAgain'].astype(int)\n",
    "\n",
    "# Handle categorical variables with one-hot encoding\n",
    "categorical_cols = ['StudentLevel', 'Discipline', 'TaskType', 'FinalOutcome']\n",
    "X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "print(f\"Original feature count: {len(feature_cols)}\")\n",
    "print(f\"Feature count after encoding: {X_encoded.shape[1]}\")\n",
    "print(f\"Target variable distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"\\nClass imbalance ratio: {y.value_counts()[0] / y.value_counts()[1]:.3f}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Scale numerical features\n",
    "numerical_cols_ml = [col for col in X_encoded.columns \n",
    "                     if X_encoded[col].dtype in [np.float64, np.int64] \n",
    "                     and col not in ['HighAssistance', 'HighSatisfaction', 'LongSession', 'SuccessfulOutcome']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "X_train_scaled[numerical_cols_ml] = scaler.fit_transform(X_train[numerical_cols_ml])\n",
    "X_test_scaled[numerical_cols_ml] = scaler.transform(X_test[numerical_cols_ml])\n",
    "\n",
    "print(f\"\\nScaling applied to {len(numerical_cols_ml)} numerical features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate multiple baseline models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "    'LightGBM': lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {name}...\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Use scaled data for Logistic Regression, original for tree-based\n",
    "    if name == 'Logistic Regression':\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc\n",
    "    })\n",
    "    \n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('ROC-AUC', ascending=False)\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "display(results_df.round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Model Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar plot of metrics\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.15\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    axes[0].bar(x + i*width, results_df[metric], width, label=metric, alpha=0.8)\n",
    "\n",
    "axes[0].set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x + width * 2)\n",
    "axes[0].set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "axes[0].set_ylim([0, 1])\n",
    "\n",
    "# ROC-AUC comparison\n",
    "axes[1].barh(results_df['Model'], results_df['ROC-AUC'], color='#339af0', edgecolor='black')\n",
    "axes[1].set_xlabel('ROC-AUC Score', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('ROC-AUC by Model', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "axes[1].set_xlim([0, 1])\n",
    "\n",
    "for i, v in enumerate(results_df['ROC-AUC']):\n",
    "    axes[1].text(v + 0.01, i, f'{v:.4f}', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 ROC Curves for Best Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for top 3 models\n",
    "top_models = results_df.head(3)['Model'].tolist()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "for name in top_models:\n",
    "    model = models[name]\n",
    "    \n",
    "    if name == 'Logistic Regression':\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    ax.plot(fpr, tpr, linewidth=2, label=f'{name} (AUC = {roc_auc:.4f})')\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_title('ROC Curves - Top 3 Models', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right', fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7 Cross-Validation Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation for model robustness assessment\n",
    "print(\"=\"*70)\n",
    "print(\"CROSS-VALIDATION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_results = []\n",
    "\n",
    "# Evaluate top 3 models with cross-validation\n",
    "top_3_models = results_df.head(3)['Model'].tolist()\n",
    "\n",
    "for name in top_3_models:\n",
    "    print(f\"\\n{name} - Cross-Validation:\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    if name == 'Logistic Regression':\n",
    "        model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "        X_cv = X_train_scaled\n",
    "    elif name == 'Random Forest':\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        X_cv = X_train\n",
    "    elif name == 'XGBoost':\n",
    "        model = xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "        X_cv = X_train\n",
    "    elif name == 'LightGBM':\n",
    "        model = lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
    "        X_cv = X_train\n",
    "    elif name == 'Gradient Boosting':\n",
    "        model = GradientBoostingClassifier(random_state=42)\n",
    "        X_cv = X_train\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # Cross-validation scores\n",
    "    cv_scores_auc = cross_val_score(model, X_cv, y_train, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "    cv_scores_acc = cross_val_score(model, X_cv, y_train, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "    cv_scores_f1 = cross_val_score(model, X_cv, y_train, cv=cv, scoring='f1', n_jobs=-1)\n",
    "    \n",
    "    cv_results.append({\n",
    "        'Model': name,\n",
    "        'CV_ROC-AUC_Mean': cv_scores_auc.mean(),\n",
    "        'CV_ROC-AUC_Std': cv_scores_auc.std(),\n",
    "        'CV_Accuracy_Mean': cv_scores_acc.mean(),\n",
    "        'CV_Accuracy_Std': cv_scores_acc.std(),\n",
    "        'CV_F1_Mean': cv_scores_f1.mean(),\n",
    "        'CV_F1_Std': cv_scores_f1.std()\n",
    "    })\n",
    "    \n",
    "    print(f\"ROC-AUC: {cv_scores_auc.mean():.4f} (+/- {cv_scores_auc.std() * 2:.4f})\")\n",
    "    print(f\"Accuracy: {cv_scores_acc.mean():.4f} (+/- {cv_scores_acc.std() * 2:.4f})\")\n",
    "    print(f\"F1-Score: {cv_scores_f1.mean():.4f} (+/- {cv_scores_f1.std() * 2:.4f})\")\n",
    "    print(f\"CV Scores (ROC-AUC): {cv_scores_auc}\")\n",
    "\n",
    "cv_results_df = pd.DataFrame(cv_results)\n",
    "print(\"\\n\\n\" + \"=\"*70)\n",
    "print(\"CROSS-VALIDATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "display(cv_results_df.round(4))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "metrics_cv = ['CV_ROC-AUC_Mean', 'CV_Accuracy_Mean', 'CV_F1_Mean']\n",
    "metric_labels = ['ROC-AUC', 'Accuracy', 'F1-Score']\n",
    "\n",
    "for idx, (metric, label) in enumerate(zip(metrics_cv, metric_labels)):\n",
    "    means = cv_results_df[metric]\n",
    "    stds = cv_results_df[metric.replace('_Mean', '_Std')]\n",
    "    \n",
    "    axes[idx].barh(cv_results_df['Model'], means, xerr=stds, \n",
    "                   capsize=5, alpha=0.7, edgecolor='black')\n",
    "    axes[idx].set_xlabel(f'{label} Score', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_title(f'Cross-Validation {label}', fontsize=14, fontweight='bold')\n",
    "    axes[idx].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    for i, (m, s) in enumerate(zip(means, stds)):\n",
    "        axes[idx].text(m + s + 0.01, i, f'{m:.3f}\u00b1{s:.3f}', \n",
    "                      va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.8 Model Calibration Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Calibration Analysis\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL CALIBRATION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Select best model for calibration analysis\n",
    "best_model_name_cv = cv_results_df.loc[cv_results_df['CV_ROC-AUC_Mean'].idxmax(), 'Model']\n",
    "print(f\"\\nAnalyzing calibration for: {best_model_name_cv}\")\n",
    "\n",
    "# Train and get predictions\n",
    "if best_model_name_cv == 'Logistic Regression':\n",
    "    best_model_cal = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    best_model_cal.fit(X_train_scaled, y_train)\n",
    "    y_proba_cal = best_model_cal.predict_proba(X_test_scaled)[:, 1]\n",
    "elif best_model_name_cv == 'Random Forest':\n",
    "    best_model_cal = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    best_model_cal.fit(X_train, y_train)\n",
    "    y_proba_cal = best_model_cal.predict_proba(X_test)[:, 1]\n",
    "elif best_model_name_cv == 'XGBoost':\n",
    "    best_model_cal = xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "    best_model_cal.fit(X_train, y_train)\n",
    "    y_proba_cal = best_model_cal.predict_proba(X_test)[:, 1]\n",
    "else:\n",
    "    best_model_cal = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    best_model_cal.fit(X_train, y_train)\n",
    "    y_proba_cal = best_model_cal.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calibration curve\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "    y_test, y_proba_cal, n_bins=10, strategy='uniform'\n",
    ")\n",
    "\n",
    "# Brier score\n",
    "brier_score = brier_score_loss(y_test, y_proba_cal)\n",
    "\n",
    "# Plot calibration curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Calibration curve\n",
    "axes[0].plot(mean_predicted_value, fraction_of_positives, \"s-\", \n",
    "             label=f'{best_model_name_cv}', linewidth=2, markersize=8)\n",
    "axes[0].plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\", linewidth=2)\n",
    "axes[0].set_xlabel('Mean Predicted Probability', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Fraction of Positives', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title(f'Calibration Curve - {best_model_name_cv}', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Histogram of predicted probabilities\n",
    "axes[1].hist(y_proba_cal, bins=20, edgecolor='black', alpha=0.7, color='#339af0')\n",
    "axes[1].axvline(y_proba_cal.mean(), color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Mean: {y_proba_cal.mean():.3f}')\n",
    "axes[1].set_xlabel('Predicted Probability', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title(f'Predicted Probability Distribution - {best_model_name_cv}', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBrier Score: {brier_score:.4f}\")\n",
    "print(\"(Lower is better; 0 = perfect calibration, 1 = worst)\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "if brier_score < 0.1:\n",
    "    print(\"Excellent calibration - model predictions are well-calibrated\")\n",
    "elif brier_score < 0.2:\n",
    "    print(\"Good calibration - model predictions are reasonably reliable\")\n",
    "else:\n",
    "    print(\"Poor calibration - model predictions may not be reliable\")\n",
    "\n",
    "# Calibrated model (using Platt scaling)\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CALIBRATED MODEL (Platt Scaling)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if best_model_name_cv == 'Logistic Regression':\n",
    "    calibrated_model = CalibratedClassifierCV(best_model_cal, method='sigmoid', cv=3)\n",
    "    calibrated_model.fit(X_train_scaled, y_train)\n",
    "    y_proba_calibrated = calibrated_model.predict_proba(X_test_scaled)[:, 1]\n",
    "else:\n",
    "    calibrated_model = CalibratedClassifierCV(best_model_cal, method='isotonic', cv=3)\n",
    "    calibrated_model.fit(X_train, y_train)\n",
    "    y_proba_calibrated = calibrated_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "brier_score_calibrated = brier_score_loss(y_test, y_proba_calibrated)\n",
    "fraction_of_positives_cal, mean_predicted_value_cal = calibration_curve(\n",
    "    y_test, y_proba_calibrated, n_bins=10, strategy='uniform'\n",
    ")\n",
    "\n",
    "print(f\"Brier Score (Calibrated): {brier_score_calibrated:.4f}\")\n",
    "print(f\"Improvement: {brier_score - brier_score_calibrated:.4f}\")\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(mean_predicted_value, fraction_of_positives, \"s-\", \n",
    "         label=f'{best_model_name_cv} (Original)', linewidth=2, markersize=8)\n",
    "plt.plot(mean_predicted_value_cal, fraction_of_positives_cal, \"o-\", \n",
    "         label=f'{best_model_name_cv} (Calibrated)', linewidth=2, markersize=8)\n",
    "plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\", linewidth=2)\n",
    "plt.xlabel('Mean Predicted Probability', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Fraction of Positives', fontsize=12, fontweight='bold')\n",
    "plt.title('Calibration Comparison: Original vs Calibrated', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.9 SHAP Values for Model Interpretability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Values for Model Interpretability\n",
    "try:\n",
    "    import shap\n",
    "    shap_available = True\n",
    "except ImportError:\n",
    "    shap_available = False\n",
    "    print(\"SHAP library not available. Install with: pip install shap\")\n",
    "\n",
    "if shap_available:\n",
    "    print(\"=\"*70)\n",
    "    print(\"SHAP VALUE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Use best tree-based model for SHAP (works best with tree models)\n",
    "    if best_model_name_cv in ['Random Forest', 'XGBoost', 'LightGBM', 'Gradient Boosting']:\n",
    "        if best_model_name_cv == 'Random Forest':\n",
    "            shap_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "            shap_model.fit(X_train, y_train)\n",
    "            X_shap = X_train\n",
    "            explainer = shap.TreeExplainer(shap_model)\n",
    "        elif best_model_name_cv == 'XGBoost':\n",
    "            shap_model = xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "            shap_model.fit(X_train, y_train)\n",
    "            X_shap = X_train\n",
    "            explainer = shap.TreeExplainer(shap_model)\n",
    "        elif best_model_name_cv == 'LightGBM':\n",
    "            shap_model = lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
    "            shap_model.fit(X_train, y_train)\n",
    "            X_shap = X_train\n",
    "            explainer = shap.TreeExplainer(shap_model)\n",
    "        else:\n",
    "            shap_model = GradientBoostingClassifier(random_state=42)\n",
    "            shap_model.fit(X_train, y_train)\n",
    "            X_shap = X_train\n",
    "            explainer = shap.TreeExplainer(shap_model)\n",
    "        \n",
    "        # Calculate SHAP values (use sample for efficiency)\n",
    "        sample_size = min(500, len(X_test))\n",
    "        X_shap_sample = X_shap.sample(n=sample_size, random_state=42)\n",
    "        shap_values = explainer.shap_values(X_shap_sample)\n",
    "        \n",
    "        # For binary classification, use class 1 (UsedAgain=True)\n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values_display = shap_values[1]\n",
    "        else:\n",
    "            shap_values_display = shap_values\n",
    "        \n",
    "        print(f\"\\nSHAP values calculated for {best_model_name_cv}\")\n",
    "        print(f\"Sample size: {sample_size} observations\")\n",
    "        print(f\"Number of features: {X_shap_sample.shape[1]}\")\n",
    "        \n",
    "        # Summary plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(shap_values_display, X_shap_sample, \n",
    "                         show=False, max_display=20, plot_size=(12, 8))\n",
    "        plt.title(f'SHAP Summary Plot - {best_model_name_cv}', \n",
    "                 fontsize=14, fontweight='bold', pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Bar plot of mean SHAP values\n",
    "        shap.summary_plot(shap_values_display, X_shap_sample, plot_type=\"bar\", \n",
    "                         show=False, max_display=20)\n",
    "        plt.title(f'Mean |SHAP Value| - {best_model_name_cv}', \n",
    "                 fontsize=14, fontweight='bold', pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate feature importance from SHAP\n",
    "        shap_importance = pd.DataFrame({\n",
    "            'Feature': X_shap_sample.columns,\n",
    "            'Mean_|SHAP_Value|': np.abs(shap_values_display).mean(axis=0)\n",
    "        }).sort_values('Mean_|SHAP_Value|', ascending=False).head(15)\n",
    "        \n",
    "        print(\"\\nTop 15 Features by Mean |SHAP Value|:\")\n",
    "        display(shap_importance.round(4))\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nSHAP analysis optimized for tree-based models.\")\n",
    "        print(f\"Using Random Forest as proxy for SHAP analysis...\")\n",
    "        shap_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        shap_model.fit(X_train, y_train)\n",
    "        explainer = shap.TreeExplainer(shap_model)\n",
    "        \n",
    "        sample_size = min(500, len(X_test))\n",
    "        X_shap_sample = X_train.sample(n=sample_size, random_state=42)\n",
    "        shap_values = explainer.shap_values(X_shap_sample)\n",
    "        \n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values_display = shap_values[1]\n",
    "        else:\n",
    "            shap_values_display = shap_values\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(shap_values_display, X_shap_sample, \n",
    "                         show=False, max_display=20, plot_size=(12, 8))\n",
    "        plt.title('SHAP Summary Plot - Random Forest (Proxy)', \n",
    "                 fontsize=14, fontweight='bold', pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        shap_importance = pd.DataFrame({\n",
    "            'Feature': X_shap_sample.columns,\n",
    "            'Mean_|SHAP_Value|': np.abs(shap_values_display).mean(axis=0)\n",
    "        }).sort_values('Mean_|SHAP_Value|', ascending=False).head(15)\n",
    "        \n",
    "        print(\"\\nTop 15 Features by Mean |SHAP Value|:\")\n",
    "        display(shap_importance.round(4))\n",
    "\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SHAP ANALYSIS - LIBRARY NOT INSTALLED\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nTo enable SHAP analysis, install the library:\")\n",
    "    print(\"  pip install shap\")\n",
    "    print(\"\\nSHAP provides model interpretability by explaining individual predictions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Feature Importance (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest and get feature importance\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False).head(20)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(len(feature_importance)), feature_importance['Importance'], \n",
    "         color='#339af0', edgecolor='black')\n",
    "plt.yticks(range(len(feature_importance)), feature_importance['Feature'])\n",
    "plt.xlabel('Feature Importance', fontsize=12, fontweight='bold')\n",
    "plt.title('Top 20 Most Important Features (Random Forest)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(feature_importance['Importance']):\n",
    "    plt.text(v + 0.001, i, f'{v:.4f}', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "display(feature_importance.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Hyperparameter Tuning (Best Model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for the best model (assuming XGBoost or Random Forest)\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "print(f\"Performing hyperparameter tuning for: {best_model_name}\")\n",
    "\n",
    "if best_model_name == 'XGBoost':\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    }\n",
    "    base_model = xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "elif best_model_name == 'Random Forest':\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    }\n",
    "    base_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "else:\n",
    "    print(f\"Hyperparameter tuning not implemented for {best_model_name}\")\n",
    "    param_grid = None\n",
    "\n",
    "if param_grid:\n",
    "    # Use smaller grid search for demonstration (full search takes longer)\n",
    "    print(\"Performing grid search (this may take a few minutes)...\")\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    grid_search = GridSearchCV(\n",
    "        base_model, param_grid, cv=cv, \n",
    "        scoring='roc_auc', n_jobs=-1, verbose=1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred_tuned = best_model.predict(X_test)\n",
    "    y_proba_tuned = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "    roc_auc_tuned = roc_auc_score(y_test, y_proba_tuned)\n",
    "    f1_tuned = f1_score(y_test, y_pred_tuned)\n",
    "    \n",
    "    print(f\"\\nTuned Model Performance on Test Set:\")\n",
    "    print(f\"Accuracy: {accuracy_tuned:.4f}\")\n",
    "    print(f\"ROC-AUC:  {roc_auc_tuned:.4f}\")\n",
    "    print(f\"F1-Score: {f1_tuned:.4f}\")\n",
    "    \n",
    "    print(f\"\\nImprovement over baseline:\")\n",
    "    print(f\"ROC-AUC: {roc_auc_tuned - results_df[results_df['Model']==best_model_name]['ROC-AUC'].values[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Insights and Conclusions\n",
    "\n",
    "### 8.1 Summary of Key Findings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive insights\n",
    "print(\"=\"*70)\n",
    "print(\"COMPREHENSIVE ANALYSIS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. DATASET OVERVIEW\")\n",
    "print(\"-\"*70)\n",
    "print(f\"   \u2022 Total sessions analyzed: {len(df):,}\")\n",
    "print(f\"   \u2022 Time period: {df['SessionDate'].min().strftime('%Y-%m-%d')} to {df['SessionDate'].max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"   \u2022 Student levels: {', '.join(df['StudentLevel'].unique())}\")\n",
    "print(f\"   \u2022 Task types: {', '.join(df['TaskType'].unique())}\")\n",
    "\n",
    "print(\"\\n2. TARGET VARIABLE (UsedAgain)\")\n",
    "print(\"-\"*70)\n",
    "used_again_pct = df['UsedAgain'].value_counts(normalize=True) * 100\n",
    "print(f\"   \u2022 Reuse rate: {used_again_pct[True]:.1f}% ({df['UsedAgain'].sum():,} sessions)\")\n",
    "print(f\"   \u2022 Non-reuse rate: {used_again_pct[False]:.1f}% ({(~df['UsedAgain']).sum():,} sessions)\")\n",
    "print(f\"   \u2022 Class imbalance: Moderate ({used_again_pct[False]/used_again_pct[True]:.2f}:1)\")\n",
    "\n",
    "print(\"\\n3. KEY DEMOGRAPHIC PATTERNS\")\n",
    "print(\"-\"*70)\n",
    "# Most common student level\n",
    "most_common_level = df['StudentLevel'].value_counts().index[0]\n",
    "print(f\"   \u2022 Most common student level: {most_common_level}\")\n",
    "# Most common task type\n",
    "most_common_task = df['TaskType'].value_counts().index[0]\n",
    "print(f\"   \u2022 Most common task type: {most_common_task}\")\n",
    "# Average session length\n",
    "print(f\"   \u2022 Average session length: {df['SessionLengthMin'].mean():.1f} minutes\")\n",
    "print(f\"   \u2022 Average prompts per session: {df['TotalPrompts'].mean():.1f}\")\n",
    "\n",
    "print(\"\\n4. STATISTICAL SIGNIFICANCE\")\n",
    "print(\"-\"*70)\n",
    "# Check which tests were significant\n",
    "print(\"   Significant associations with UsedAgain:\")\n",
    "sig_vars = []\n",
    "if len(chi2_df) > 0:\n",
    "    for _, row in chi2_df.iterrows():\n",
    "        if row['Significant'] == 'Yes':\n",
    "            sig_vars.append(f\"{row['Variable']} (\u03c7\u00b2)\")\n",
    "if len(test_df) > 0:\n",
    "    for _, row in test_df.iterrows():\n",
    "        if row['Significant (t-test)'] == 'Yes':\n",
    "            sig_vars.append(f\"{row['Variable']} (t-test)\")\n",
    "\n",
    "if sig_vars:\n",
    "    for var in sig_vars[:5]:  # Show first 5\n",
    "        print(f\"     - {var}\")\n",
    "else:\n",
    "    print(\"     (Run statistical tests section for details)\")\n",
    "\n",
    "print(\"\\n5. MACHINE LEARNING PERFORMANCE\")\n",
    "print(\"-\"*70)\n",
    "if len(results_df) > 0:\n",
    "    best_model = results_df.iloc[0]\n",
    "    print(f\"   \u2022 Best performing model: {best_model['Model']}\")\n",
    "    print(f\"   \u2022 Accuracy: {best_model['Accuracy']:.1%}\")\n",
    "    print(f\"   \u2022 ROC-AUC: {best_model['ROC-AUC']:.3f}\")\n",
    "    print(f\"   \u2022 F1-Score: {best_model['F1-Score']:.3f}\")\n",
    "else:\n",
    "    print(\"     (Run ML models section for details)\")\n",
    "\n",
    "print(\"\\n6. KEY CORRELATIONS\")\n",
    "print(\"-\"*70)\n",
    "corr_with_used = correlation_matrix['UsedAgain'].drop('UsedAgain').abs().sort_values(ascending=False)\n",
    "if len(corr_with_used) > 0:\n",
    "    print(f\"   \u2022 Strongest predictor: {corr_with_used.index[0]} (r={corr_with_used.iloc[0]:.3f})\")\n",
    "\n",
    "print(\"\\n7. PRACTICAL IMPLICATIONS\")\n",
    "print(\"-\"*70)\n",
    "print(\"   \u2022 High reuse rate suggests AI assistants are generally well-received\")\n",
    "print(\"   \u2022 Session characteristics and outcomes are key predictors\")\n",
    "print(\"   \u2022 Feature engineering improved model interpretability\")\n",
    "print(\"   \u2022 Models can help identify at-risk user segments\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "**Analysis completed!** This notebook provides a comprehensive foundation for manuscript preparation and further research.",
    "",
    "---",
    "",
    "## Citation and Attribution",
    "",
    "**Notebook Author**: Eman Toraih  ",
    "**Date Created**: January 5, 2026  ",
    "**Analysis Type**: Comprehensive statistical analysis with machine learning modeling",
    "",
    "**Key Features**:",
    "- Rigorous statistical hypothesis testing with effect sizes",
    "- Temporal trend analysis",
    "- Cross-validated machine learning models",
    "- Model calibration assessment",
    "- SHAP values for interpretability",
    "- Publication-ready methodology",
    "",
    "**Contact**: emantoraih@outlook.com; toraihe@upstate.edu",
    "",
    "For questions or collaborations, please refer to the GitHub repository or contact the author.",
    ""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}